{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7218110,"sourceType":"datasetVersion","datasetId":4177530}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Get the API key from here: https://ai.google.dev/tutorials/setup\n# Create a new secret called \"GEMINI_API_KEY\" via Add-ons -> Secrets in the top menu, and attach it to this notebook.\nfrom kaggle_secrets import UserSecretsClient\nfrom IPython.display import display\nfrom IPython.display import Markdown\n\nimport pathlib\nimport textwrap\n\nuser_secrets = UserSecretsClient()\napiKey = user_secrets.get_secret(\"GEMINI_API_KEY\")\n\ndef to_markdown(text):\n  text = text.replace('â€¢', '  *')\n  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))","metadata":{"_uuid":"061c2e2d-8c74-472f-a9d4-d9ec92022bfa","_cell_guid":"03d3df50-d945-4192-aa17-f88262b8e5f6","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-17T05:06:18.473058Z","iopub.execute_input":"2023-12-17T05:06:18.473610Z","iopub.status.idle":"2023-12-17T05:06:18.760759Z","shell.execute_reply.started":"2023-12-17T05:06:18.473576Z","shell.execute_reply":"2023-12-17T05:06:18.760057Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import google.generativeai as genai\n\ngenai.configure(api_key = apiKey)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T05:09:15.201969Z","iopub.execute_input":"2023-12-17T05:09:15.202360Z","iopub.status.idle":"2023-12-17T05:09:15.676381Z","shell.execute_reply.started":"2023-12-17T05:09:15.202325Z","shell.execute_reply":"2023-12-17T05:09:15.675161Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"### Generate text from text-only inputs using `gemini-pro` ###\n\nThe `generate_content` method can handle a wide variety of use cases, including multi-turn chat and multimodal input, depending on what the underlying model supports. The available models only support text and images as input, and text as output.\n\nIn the simplest case, you can pass a prompt string to the `GenerativeModel.generate_content` method:","metadata":{}},{"cell_type":"code","source":"level = input()","metadata":{"execution":{"iopub.status.busy":"2023-12-17T05:09:19.394769Z","iopub.execute_input":"2023-12-17T05:09:19.395253Z","iopub.status.idle":"2023-12-17T05:09:39.201687Z","shell.execute_reply.started":"2023-12-17T05:09:19.395223Z","shell.execute_reply":"2023-12-17T05:09:39.200320Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdin","text":" Adult\n"}]},{"cell_type":"code","source":"number_of_words = input()","metadata":{"execution":{"iopub.status.busy":"2023-12-17T05:09:43.096813Z","iopub.execute_input":"2023-12-17T05:09:43.097201Z","iopub.status.idle":"2023-12-17T05:09:47.162319Z","shell.execute_reply.started":"2023-12-17T05:09:43.097174Z","shell.execute_reply":"2023-12-17T05:09:47.161369Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdin","text":" 600\n"}]},{"cell_type":"code","source":"how_many_errors = input()","metadata":{"execution":{"iopub.status.busy":"2023-12-17T05:10:26.334333Z","iopub.execute_input":"2023-12-17T05:10:26.334698Z","iopub.status.idle":"2023-12-17T05:10:29.374367Z","shell.execute_reply.started":"2023-12-17T05:10:26.334662Z","shell.execute_reply":"2023-12-17T05:10:29.373012Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdin","text":" none\n"}]},{"cell_type":"code","source":"topic = input()","metadata":{"execution":{"iopub.status.busy":"2023-12-17T05:09:55.057536Z","iopub.execute_input":"2023-12-17T05:09:55.058216Z","iopub.status.idle":"2023-12-17T05:10:24.035807Z","shell.execute_reply.started":"2023-12-17T05:09:55.058163Z","shell.execute_reply":"2023-12-17T05:10:24.035075Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdin","text":" How shakespear influences modern society\n"}]},{"cell_type":"code","source":"model = genai.GenerativeModel('gemini-pro')\nresponse = model.generate_content(f\"I have an assignment which should seem like it is made by a {level},  it should have atleast {number_of_words} characters and should contain {how_many_errors} errors. The topic is '{topic}.'\")\nto_markdown(response.text)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T05:10:31.799458Z","iopub.execute_input":"2023-12-17T05:10:31.800067Z","iopub.status.idle":"2023-12-17T05:10:40.582812Z","shell.execute_reply.started":"2023-12-17T05:10:31.800035Z","shell.execute_reply":"2023-12-17T05:10:40.581486Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"> William Shakespeare, the legendary playwright and poet of the Elizabethan era, has left an indelible mark on modern society, shaping cultural norms, language, and artistic expression. His influence transcends time and geographical boundaries, permeating various aspects of contemporary life.\n> \n> 1. **Literary Impact:**\n> \n>    - Shakespeare's literary prowess has revolutionized storytelling techniques and influenced generations of writers. His plays introduced complex characters, intricate plots, and poetic language, setting new standards for dramatic writing.\n> \n> 2. **Language and Vocabulary:**\n> \n>    - Shakespeare's vast vocabulary and innovative use of language have significantly enriched the English language. Many of his coined words and phrases have become an integral part of everyday speech, expanding our expressive capabilities.\n> \n> 3. **Cultural and Social Commentary:**\n> \n>    - Shakespeare's plays often addressed social and political issues of his time, making them relevant to modern audiences as well. His exploration of themes such as love, loss, power, and justice resonates with people across cultures and generations.\n> \n> 4. **Theatrical Legacy:**\n> \n>    - Shakespeare's plays have had a profound impact on the development of theater as an art form. His works continue to be performed worldwide, inspiring new interpretations and artistic adaptations that keep his stories alive for contemporary audiences.\n> \n> 5. **Film and Television Adaptations:**\n> \n>    - Shakespeare's plays have been adapted into countless films and television shows, reaching a vast audience beyond the traditional theater-going crowd. These adaptations have introduced Shakespeare's work to new generations and made his stories accessible to a broader spectrum of viewers.\n> \n> 6. **Educational Value:**\n> \n>    - Shakespeare's plays are widely studied in schools and universities around the world. His works offer valuable insights into human nature, history, and cultural heritage, making them essential texts for students of literature, drama, and history.\n> \n> 7. **Cultural Icons:**\n> \n>    - Shakespeare's characters have become cultural icons, recognized and referenced in popular culture. Characters like Hamlet, Romeo and Juliet, and Macbeth have transcended their literary origins and become symbols of universal human experiences.\n> \n> 8. **Philosophical and Ethical Insights:**\n> \n>    - Shakespeare's plays explore profound philosophical and ethical dilemmas, inviting audiences to reflect on the complexities of human existence. His works encourage critical thinking and introspection, challenging societal norms and encouraging empathy.\n> \n> In conclusion, Shakespeare's influence on modern society is undeniable. His works continue to shape cultural narratives, enrich our language, and inspire artistic expression across various mediums. By exploring timeless themes, creating memorable characters, and using innovative language, Shakespeare has left an enduring legacy that continues to resonate with audiences worldwide."},"metadata":{}}]},{"cell_type":"code","source":"model = genai.GenerativeModel('gemini-pro')\nresponse = model.generate_content(f\"How to convert .svg to syg font?\")\nto_markdown(response.text)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T03:35:34.965392Z","iopub.execute_input":"2023-12-17T03:35:34.965893Z","iopub.status.idle":"2023-12-17T03:35:36.999548Z","shell.execute_reply.started":"2023-12-17T03:35:34.965857Z","shell.execute_reply":"2023-12-17T03:35:36.997864Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"> Converting an SVG file to a SYG font is not possible. The .svg file extension represents Scalable Vector Graphics, a format used for two-dimensional graphics. On the other hand, the .syg extension is associated with a different file format, such as Synaptics Touchpad Driver or Symantec Generic License Agreement files. Therefore, there is no direct conversion process between these two file formats."},"metadata":{}}]},{"cell_type":"code","source":"# View the response candidates\nresponse.candidates","metadata":{"execution":{"iopub.status.busy":"2023-12-13T19:26:43.443226Z","iopub.execute_input":"2023-12-13T19:26:43.44367Z","iopub.status.idle":"2023-12-13T19:26:43.451937Z","shell.execute_reply.started":"2023-12-13T19:26:43.443636Z","shell.execute_reply":"2023-12-13T19:26:43.450838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# If API failed to return a result, use prompt_feedback to see if it was blocked due to safety concerns regarding the prompt.\nresponse.prompt_feedback","metadata":{"execution":{"iopub.status.busy":"2023-12-13T19:26:49.69715Z","iopub.execute_input":"2023-12-13T19:26:49.698073Z","iopub.status.idle":"2023-12-13T19:26:49.704055Z","shell.execute_reply.started":"2023-12-13T19:26:49.698035Z","shell.execute_reply":"2023-12-13T19:26:49.70307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Generate text from image and text prompts using `gemini-pro-vision`","metadata":{}},{"cell_type":"code","source":"!curl -o image.jpg https://t0.gstatic.com/licensed-image?q=tbn:ANd9GcQ_Kevbk21QBRy-PgB4kQpS79brbmmEG7m3VOTShAn4PecDU5H5UxrJxE3Dw1JiaG17V88QIol19-3TM2wCHw","metadata":{"execution":{"iopub.status.busy":"2023-12-13T19:26:52.331542Z","iopub.execute_input":"2023-12-13T19:26:52.331905Z","iopub.status.idle":"2023-12-13T19:26:53.461896Z","shell.execute_reply.started":"2023-12-13T19:26:52.331876Z","shell.execute_reply":"2023-12-13T19:26:53.460329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import PIL.Image\n\nimg = PIL.Image.open('image.jpg')\nimg","metadata":{"execution":{"iopub.status.busy":"2023-12-13T19:26:56.827848Z","iopub.execute_input":"2023-12-13T19:26:56.828251Z","iopub.status.idle":"2023-12-13T19:26:58.202472Z","shell.execute_reply.started":"2023-12-13T19:26:56.828216Z","shell.execute_reply":"2023-12-13T19:26:58.200454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = genai.GenerativeModel('gemini-pro-vision')","metadata":{"execution":{"iopub.status.busy":"2023-12-13T19:27:03.786933Z","iopub.execute_input":"2023-12-13T19:27:03.787998Z","iopub.status.idle":"2023-12-13T19:27:03.793378Z","shell.execute_reply.started":"2023-12-13T19:27:03.787955Z","shell.execute_reply":"2023-12-13T19:27:03.792256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"response = model.generate_content(img)\n\nto_markdown(response.text)","metadata":{"execution":{"iopub.status.busy":"2023-12-13T19:27:04.632384Z","iopub.execute_input":"2023-12-13T19:27:04.632771Z","iopub.status.idle":"2023-12-13T19:27:15.889688Z","shell.execute_reply.started":"2023-12-13T19:27:04.63274Z","shell.execute_reply":"2023-12-13T19:27:15.888543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"response = model.generate_content([\"Write a short, engaging blog post based on this picture. It should include a description of the meal in the photo and talk about my journey meal prepping.\", img])\nto_markdown(response.text)","metadata":{"execution":{"iopub.status.busy":"2023-12-13T19:27:50.403097Z","iopub.execute_input":"2023-12-13T19:27:50.404071Z","iopub.status.idle":"2023-12-13T19:28:05.510098Z","shell.execute_reply.started":"2023-12-13T19:27:50.404028Z","shell.execute_reply":"2023-12-13T19:28:05.508887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Chat conversations ###\nGemini enables you to have freeform conversations across multiple turns. The `ChatSession` class simplifies the process by managing the state of the conversation, so unlike with `generate_content`, you do not have to store the conversation history as a list.","metadata":{}},{"cell_type":"code","source":"model = genai.GenerativeModel('gemini-pro')\nchat = model.start_chat(history=[])\nchat","metadata":{"execution":{"iopub.status.busy":"2023-12-13T19:28:07.136545Z","iopub.execute_input":"2023-12-13T19:28:07.136942Z","iopub.status.idle":"2023-12-13T19:28:07.144465Z","shell.execute_reply.started":"2023-12-13T19:28:07.136912Z","shell.execute_reply":"2023-12-13T19:28:07.143261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"response = chat.send_message(\"In one sentence, explain how a computer works to a young child.\")\nto_markdown(response.text)","metadata":{"execution":{"iopub.status.busy":"2023-12-13T19:28:09.811654Z","iopub.execute_input":"2023-12-13T19:28:09.812028Z","iopub.status.idle":"2023-12-13T19:28:11.00492Z","shell.execute_reply.started":"2023-12-13T19:28:09.811999Z","shell.execute_reply":"2023-12-13T19:28:11.003741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"chat.history","metadata":{"execution":{"iopub.status.busy":"2023-12-13T19:28:14.193197Z","iopub.execute_input":"2023-12-13T19:28:14.193694Z","iopub.status.idle":"2023-12-13T19:28:14.200957Z","shell.execute_reply.started":"2023-12-13T19:28:14.193655Z","shell.execute_reply":"2023-12-13T19:28:14.199782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"response = chat.send_message(\"Okay, how about a more detailed explanation to a high schooler?\")\nto_markdown(response.text)","metadata":{"execution":{"iopub.status.busy":"2023-12-13T19:28:15.537896Z","iopub.execute_input":"2023-12-13T19:28:15.538725Z","iopub.status.idle":"2023-12-13T19:28:19.469477Z","shell.execute_reply.started":"2023-12-13T19:28:15.538681Z","shell.execute_reply":"2023-12-13T19:28:19.468321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for message in chat.history:\n  display(to_markdown(f'**{message.role}**: {message.parts[0].text}'))","metadata":{"execution":{"iopub.status.busy":"2023-12-13T19:28:22.110314Z","iopub.execute_input":"2023-12-13T19:28:22.110725Z","iopub.status.idle":"2023-12-13T19:28:22.125252Z","shell.execute_reply.started":"2023-12-13T19:28:22.110691Z","shell.execute_reply":"2023-12-13T19:28:22.124006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Use embeddings ###\n\n[Embedding](https://developers.google.com/machine-learning/glossary#embedding-vector) is a technique used to represent information as a list of floating point numbers in an array. With Gemini, you can represent text (words, sentences, and blocks of text) in a vectorized form, making it easier to compare and contrast embeddings. For example, two texts that share a similar subject matter or sentiment should have similar embeddings, which can be identified through mathematical comparison techniques such as cosine similarity.\n\nUse the `embed_content` method to generate embeddings. The method handles embedding for the following tasks (`task_type`):\n\nTask Type | Description\n---       | ---\nRETRIEVAL_QUERY\t| Specifies the given text is a query in a search/retrieval setting.\nRETRIEVAL_DOCUMENT | Specifies the given text is a document in a search/retrieval setting. Using this task type requires a `title`.\nSEMANTIC_SIMILARITY\t| Specifies the given text will be used for Semantic Textual Similarity (STS).\nCLASSIFICATION\t| Specifies that the embeddings will be used for classification.\nCLUSTERING\t| Specifies that the embeddings will be used for clustering.\n\nThe following generates an embedding for a single string for document retrieval:","metadata":{}},{"cell_type":"code","source":"result = genai.embed_content(\n    model=\"models/embedding-001\",\n    content=\"What is the meaning of life?\",\n    task_type=\"retrieval_document\",\n    title=\"Embedding of single string\")\n\n# 1 input > 1 vector output\nprint(str(result['embedding'])[:50], '... TRIMMED]')","metadata":{"execution":{"iopub.status.busy":"2023-12-13T19:28:29.338585Z","iopub.execute_input":"2023-12-13T19:28:29.338966Z","iopub.status.idle":"2023-12-13T19:28:29.764288Z","shell.execute_reply.started":"2023-12-13T19:28:29.338938Z","shell.execute_reply":"2023-12-13T19:28:29.7631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note: The `retrieval_document` task type is the only task that accepts a title.\n\nTo handle batches of strings, pass a list of strings in `content`:","metadata":{}},{"cell_type":"code","source":"result = genai.embed_content(\n    model=\"models/embedding-001\",\n    content=[\n      'What is the meaning of life?',\n      'How much wood would a woodchuck chuck?',\n      'How does the brain work?'],\n    task_type=\"retrieval_document\",\n    title=\"Embedding of list of strings\")\n\n# A list of inputs > A list of vectors output\nfor v in result['embedding']:\n  print(str(v)[:50], '... TRIMMED ...')","metadata":{"execution":{"iopub.status.busy":"2023-12-13T19:28:36.308553Z","iopub.execute_input":"2023-12-13T19:28:36.308922Z","iopub.status.idle":"2023-12-13T19:28:36.715298Z","shell.execute_reply.started":"2023-12-13T19:28:36.308893Z","shell.execute_reply":"2023-12-13T19:28:36.714054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"While the `genai.embed_content` function accepts simple strings or lists of strings, it is actually built around the `glm.Content` type (like `GenerativeModel.generate_content`). `glm.Content` objects are the primary units of conversation in the API.\n\nWhile the `glm.Content` object is multimodal, the `embed_content` method only supports text embeddings. This design gives the API the *possibility* to expand to multimodal embeddings.","metadata":{}},{"cell_type":"code","source":"response.candidates[0].content","metadata":{"execution":{"iopub.status.busy":"2023-12-13T19:28:53.812391Z","iopub.execute_input":"2023-12-13T19:28:53.81281Z","iopub.status.idle":"2023-12-13T19:28:53.819387Z","shell.execute_reply.started":"2023-12-13T19:28:53.812776Z","shell.execute_reply":"2023-12-13T19:28:53.818623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = genai.embed_content(\n    model = 'models/embedding-001',\n    content = response.candidates[0].content)\n\n# 1 input > 1 vector output\nprint(str(result['embedding'])[:50], '... TRIMMED ...')","metadata":{"execution":{"iopub.status.busy":"2023-12-13T19:28:55.090988Z","iopub.execute_input":"2023-12-13T19:28:55.092022Z","iopub.status.idle":"2023-12-13T19:28:55.497162Z","shell.execute_reply.started":"2023-12-13T19:28:55.091982Z","shell.execute_reply":"2023-12-13T19:28:55.495834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"chat.history","metadata":{"execution":{"iopub.status.busy":"2023-12-13T19:28:57.931921Z","iopub.execute_input":"2023-12-13T19:28:57.932646Z","iopub.status.idle":"2023-12-13T19:28:57.940232Z","shell.execute_reply.started":"2023-12-13T19:28:57.932598Z","shell.execute_reply":"2023-12-13T19:28:57.939367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = genai.embed_content(\n    model = 'models/embedding-001',\n    content = chat.history)\n\n# 1 input > 1 vector output\nfor i,v in enumerate(result['embedding']):\n  print(str(v)[:50], '... TRIMMED...')","metadata":{"execution":{"iopub.status.busy":"2023-12-13T19:28:59.143065Z","iopub.execute_input":"2023-12-13T19:28:59.144354Z","iopub.status.idle":"2023-12-13T19:28:59.618004Z","shell.execute_reply.started":"2023-12-13T19:28:59.144311Z","shell.execute_reply":"2023-12-13T19:28:59.61675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Encode messages\nThe previous sections relied on the SDK to make it easy for you to send prompts to the API. This section offers a fully-typed equivalent to the previous example, so you can better understand the lower-level details regarding how the SDK encodes messages.\n\nUnderlying the Python SDK is the `google.ai.generativelanguage` client library:","metadata":{}},{"cell_type":"code","source":"import google.ai.generativelanguage as glm","metadata":{"execution":{"iopub.status.busy":"2023-12-13T19:29:02.062468Z","iopub.execute_input":"2023-12-13T19:29:02.062851Z","iopub.status.idle":"2023-12-13T19:29:02.067968Z","shell.execute_reply.started":"2023-12-13T19:29:02.062822Z","shell.execute_reply":"2023-12-13T19:29:02.066605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The SDK attempts to convert your message to a `glm.Content` object, which contains a list of `glm.Part` objects that each contain either:\n\n1. a `text` (string)\n2. `inline_data` (`glm.Blob`), where a blob contains binary `data` and a `mime_type`.\n\nYou can also pass any of these classes as an equivalent dictionary.\n\nNote: The only accepted mime types are some image types, `image/*`.\n\nSo, the fully-typed equivalent to the previous example is:  ","metadata":{}},{"cell_type":"code","source":"model = genai.GenerativeModel('gemini-pro-vision')\nresponse = model.generate_content(\n    glm.Content(\n        parts = [\n            glm.Part(text=\"Write a short, engaging blog post based on this picture.\"),\n            glm.Part(\n                inline_data=glm.Blob(\n                    mime_type='image/jpeg',\n                    data=pathlib.Path('image.jpg').read_bytes()\n                )\n            ),\n        ],\n    )\n)\nto_markdown(response.text[:100] + \"... [TRIMMED] ...\")","metadata":{"execution":{"iopub.status.busy":"2023-12-13T19:29:03.874746Z","iopub.execute_input":"2023-12-13T19:29:03.875895Z","iopub.status.idle":"2023-12-13T19:29:12.836132Z","shell.execute_reply.started":"2023-12-13T19:29:03.875845Z","shell.execute_reply":"2023-12-13T19:29:12.835068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Multi-turn conversations ###\nWhile the `genai.ChatSession` class shown earlier can handle many use cases, it does make some assumptions. If your use case doesn't fit into this chat implementation it's good to remember that `genai.ChatSession` is just a wrapper around `GenerativeModel.generate_content`. In addition to single requests, it can handle multi-turn conversations.\n\nThe individual messages are `glm.Content` objects or compatible dictionaries, as seen in previous sections. As a dictionary, the message requires `role` and `parts` keys. The `role` in a conversation can either be the `user`, which provides the prompts, or `model`, which provides the responses.\n\nPass a list of `glm.Content` objects and it will be treated as multi-turn chat:","metadata":{}},{"cell_type":"code","source":"model = genai.GenerativeModel('gemini-pro')\n\nmessages = [\n    {'role':'user',\n     'parts': [\"Briefly explain how a computer works to a young child.\"]}\n]\nresponse = model.generate_content(messages)\n\nto_markdown(response.text)","metadata":{"execution":{"iopub.status.busy":"2023-12-13T19:29:18.440904Z","iopub.execute_input":"2023-12-13T19:29:18.441448Z","iopub.status.idle":"2023-12-13T19:29:22.140593Z","shell.execute_reply.started":"2023-12-13T19:29:18.441398Z","shell.execute_reply":"2023-12-13T19:29:22.139634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To continue the conversation, add the response and another message.\n\nNote: For multi-turn conversations, you need to send the whole conversation history with each request. The API is **stateless**.","metadata":{}},{"cell_type":"code","source":"messages.append({'role':'model',\n                 'parts':[response.text]})\n\nmessages.append({'role':'user',\n                 'parts':[\"Okay, how about a more detailed explanation to a high school student?\"]})\n\nresponse = model.generate_content(messages)\n\nto_markdown(response.text)","metadata":{"execution":{"iopub.status.busy":"2023-12-13T19:29:26.415848Z","iopub.execute_input":"2023-12-13T19:29:26.416261Z","iopub.status.idle":"2023-12-13T19:29:33.257236Z","shell.execute_reply.started":"2023-12-13T19:29:26.416225Z","shell.execute_reply":"2023-12-13T19:29:33.25587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Generation configuration\n\nThe `generation_config` argument allows you to modify the generation parameters. Every prompt you send to the model includes parameter values that control how the model generates responses.","metadata":{}},{"cell_type":"code","source":"response = model.generate_content(\n    'Tell me a story about a magic backpack.',\n    generation_config=genai.types.GenerationConfig(\n        # Only one candidate for now.\n        candidate_count=1,\n        stop_sequences=['x'],\n        max_output_tokens=20,\n        temperature=1.0)\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-13T19:29:36.927253Z","iopub.execute_input":"2023-12-13T19:29:36.927689Z","iopub.status.idle":"2023-12-13T19:29:38.10504Z","shell.execute_reply.started":"2023-12-13T19:29:36.927655Z","shell.execute_reply":"2023-12-13T19:29:38.10379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}